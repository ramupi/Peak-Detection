{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gspread\n",
        "!pip install mpld3\n",
        "!pip install xlsxwriter\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)\n",
        "%matplotlib notebook\n",
        "import mpld3\n",
        "mpld3.enable_notebook()\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import math\n",
        "\n",
        "from scipy import signal\n",
        "from scipy.signal import find_peaks,peak_prominences\n",
        "from scipy.signal import butter,filtfilt\n",
        "from scipy.fft import fft, fftfreq\n",
        "from scipy.signal import cheby2\n",
        "from scipy.signal import iirnotch\n",
        "from scipy.signal import medfilt\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "from numpy import array, sign, zeros\n",
        "\n",
        "\n",
        "def get_FFT(data, fs): # fs: sampling frequency\n",
        "    N = len(data) \n",
        "    # sample spacing\n",
        "    T = 1.0 / fs\n",
        "    yf = fft(data)\n",
        "    xf = fftfreq(N, T)[:N//2] # positive frequencies of the spectrum\n",
        "    yf_mags = 2.0/N * np.abs(yf[0:N//2]) # magnitude values of frequency components\n",
        "    \n",
        "    return xf, yf_mags\n",
        "\n",
        "def adaptive_notch_filt(data, samp_freq, harmonic_freq):\n",
        "    freq, amp = get_FFT(data, samp_freq)\n",
        "    ind_range = np.where(np.logical_and(freq>= harmonic_freq - 2, freq <= harmonic_freq + 2))[0]\n",
        "    \n",
        "    max_loc = np.argmax(amp[ind_range])\n",
        "    notch_freq = np.round(freq[ind_range][max_loc], 1)\n",
        "    \n",
        "    fn = 0.5*samp_freq\n",
        "    fc = notch_freq\n",
        "    Q = 3\n",
        "    b, a = iirnotch(fc / fn, Q)\n",
        "    filt_signal = filtfilt(b, a, data, axis = 0)\n",
        "    return filt_signal\n",
        "\n",
        "def adp_notch_filter(rec_data, fs):\n",
        "    notch_filt1 = adaptive_notch_filt(rec_data, fs, 50)\n",
        "    return notch_filt1\n",
        "\n",
        "def butter_bandpass_filter(data,lowcutoff,highcutoff, fs, order):\n",
        "    nyq = 0.5 * fs # Nyquist Frequency\n",
        "    low_cutoff = lowcutoff / nyq\n",
        "    high_cutoff=highcutoff/nyq\n",
        "    # Get the filter coefficients \n",
        "    b, a = butter(order,[low_cutoff,high_cutoff], btype='bandpass', analog=False)\n",
        "    y = filtfilt(b, a,data)\n",
        "    return y\n",
        "\n",
        "def get_fft_spec(sig, samp_freq, low, high): # get freq amps values in the specific range\n",
        "    freq_magnitudes, freqs, __ = plt.magnitude_spectrum(sig, Fs = samp_freq)\n",
        "    indices = np.where((freqs>low) & (freqs<=high))[0]\n",
        "    req_freqs = freqs[indices]\n",
        "    req_specs = freq_magnitudes[indices]\n",
        "    \n",
        "    return req_specs, req_freqs\n",
        "\n",
        "def mov_avg(data, w_d):\n",
        "    return np.convolve(data, np.ones(w_d)/w_d, 'same')\n",
        "\n",
        "def findLocalMaxima(arr):\n",
        " \n",
        "    # Empty lists to store points of\n",
        "    # local maxima and minima\n",
        "    mx = []\n",
        "    max_values = []\n",
        "  \n",
        "    # Iterating over all points to check\n",
        "    # local maxima and local minima\n",
        "    n = len(arr)\n",
        "    for i in range(1, n-1):\n",
        " \n",
        "        # Condition for local maxima\n",
        "        if(arr[i-1] < arr[i] > arr[i + 1]):\n",
        "            mx.append(i)\n",
        "            max_values.append(arr[i])\n",
        "    \n",
        "    return mx\n",
        "\n",
        "def get_closeby_pnts(pnts1, pnts2, max_abs_range): # max_abs_range >=0 and less than the half of the pulse width\n",
        "    n1 = pnts1\n",
        "    n2 = pnts2\n",
        "    closeby_pnts1 = []\n",
        "    closeby_pnts2 = []\n",
        "    \n",
        "    for i in range(len(n1)):\n",
        "        for j in range(len(n2)):\n",
        "            if abs(n1[i]-n2[j])<=max_abs_range:\n",
        "                closeby_pnts1.append(n1[i])\n",
        "                closeby_pnts2.append(n2[j])\n",
        "                \n",
        "    return np.array(closeby_pnts1), np.array(closeby_pnts2)\n",
        "\n",
        "\n",
        "def get_envelopes(sig, ipln_method):\n",
        "    s = sig\n",
        "    q_u = zeros(len(sig))\n",
        "    q_l = zeros(len(sig))\n",
        "    \n",
        "    f_mags, freqs = get_fft_spec(sig, fs, 2-1.2, 2+0.9)  # get the frequency spectrum within the range 2-0.9:2+0.9\n",
        "\n",
        "    # get first harmonic frequency\n",
        "    f1_harmonic = freqs[np.argmax(f_mags)]\n",
        "    \n",
        "    PP_min = int(fs/f1_harmonic) - (fs/6)  # fs/6 has chosen based on working on data\n",
        "\n",
        "    # find out the valleys in each of the wavelength\n",
        "    y = sig\n",
        "    y_inv = max(y) - y\n",
        "    valleys,_ = find_peaks(y_inv, distance = PP_min)\n",
        "    sig_valleys = sig[valleys]\n",
        "    l_x = valleys; l_y = sig_valleys\n",
        "\n",
        "    #Prepend the first value of (s) to the interpolating values. This forces the model to use the same starting point for both the upper and lower envelope models.\n",
        "\n",
        "    u_x = []\n",
        "    u_y = []\n",
        "\n",
        "    l_x = [0] + list(l_x)\n",
        "    l_y = [np.mean(sig_valleys[:6])] + list(l_y)\n",
        "\n",
        "#     #Detect peaks and troughs and mark their location in u_x,u_y,l_x,l_y respectively.\n",
        "\n",
        "    for k in range(1,len(s)-1):\n",
        "        if (sign(s[k]-s[k-1])==1) and (sign(s[k]-s[k+1])==1):\n",
        "            u_x.append(k)\n",
        "            u_y.append(s[k])\n",
        "\n",
        "#         if (sign(s[k]-s[k-1])==-1) and ((sign(s[k]-s[k+1]))==-1):\n",
        "#             l_x.append(k)\n",
        "#             l_y.append(s[k])\n",
        "    \n",
        "    #Append the last value of (s) to the interpolating values. This forces the model to use the same ending point for both the upper and lower envelope models.\n",
        "    \n",
        "    u_x.append(len(s)-1)\n",
        "    u_y.append(s[-1])\n",
        "\n",
        "    l_x.append(len(s)-1)\n",
        "    l_y.append(np.mean(sig_valleys[-6:]))\n",
        "\n",
        "    #Fit suitable models to the data. Here I am using cubic splines, similarly to the MATLAB example given in the question.\n",
        "\n",
        "    u_p = interp1d(u_x,u_y, kind = ipln_method, bounds_error = False, fill_value=0.0)\n",
        "    l_p = interp1d(l_x,l_y, kind = ipln_method, bounds_error = False, fill_value=0.0)\n",
        "\n",
        "    #Evaluate each model over the domain of (s)\n",
        "    for k in range(0,len(sig)):\n",
        "        q_u[k] = u_p(k)\n",
        "        q_l[k] = l_p(k)\n",
        "        \n",
        "    return q_u, q_l\n",
        "\n",
        "def get_BW_removed_sig(sig, samp_freq, inpln_method):\n",
        "    upper_epe, lower_epe = get_envelopes(sig, inpln_method)\n",
        "#     dup_epe,dlow_epe = get_envelopes(lower_epe, \"linear\")\n",
        "    # # tup_epe, tlow_epe = get_envelopes(dlow_epe)\n",
        "\n",
        "    bw_removed_sig = sig - mov_avg(lower_epe, samp_freq)\n",
        "    \n",
        "    return upper_epe, lower_epe, bw_removed_sig\n",
        "\n",
        "\n",
        "def get_valleys(sig1, sig2, samp_freq):\n",
        "    \n",
        "    fs = samp_freq  # sample rate, Hz\n",
        "\n",
        "    signals = [sig1, sig2]\n",
        "\n",
        "    signals_valleys = []\n",
        "    \n",
        "    for sig in signals:\n",
        "\n",
        "        f_mags, freqs = get_fft_spec(sig, fs, 2-1.2, 2+0.9)  # get the frequency spectrum within the range 2-0.9:2+0.9\n",
        "\n",
        "        # get first harmonic frequency\n",
        "        f1_harmonic = freqs[np.argmax(f_mags)]\n",
        "\n",
        "        # [f_min, f_max] = get_harmonics_width(bpass_sigB, fs, f1_harmonic)[0]\n",
        "        # RR_min = int(fs / f_max); RR_max = int(fs / f_min);\n",
        "\n",
        "        PP_min = int(fs/f1_harmonic) - (fs/6)  # fs/6 has chosen based on working on data\n",
        "\n",
        "        # find out the valleys in each of the wavelength\n",
        "        y = sig\n",
        "        y_inv = max(y) - y\n",
        "        valleys,_ = find_peaks(y_inv, distance = PP_min)\n",
        "        \n",
        "        signals_valleys.append(valleys)\n",
        "        \n",
        "    valid_valleys1, valid_valleys2  = get_closeby_pnts(signals_valleys[0], signals_valleys[1], int(fs/(2*f1_harmonic)))\n",
        "    \n",
        "    return valid_valleys1, valid_valleys2\n",
        "\n",
        "\n",
        "def get_valid_first_peaks(vals, sig1, sig2):\n",
        "    \n",
        "    # get first peaks in each valley to valley \n",
        "    valid_first_peaks1 = []\n",
        "    valid_first_peaks2 = []\n",
        "    \n",
        "    N = len(vals)\n",
        "    \n",
        "    for v_i in range(N-1):\n",
        "        \n",
        "        pulse_peaks1 = sorted(findLocalMaxima(sig1[vals[v_i]:vals[v_i + 1]]))\n",
        "        pulse_peaks2 = sorted(findLocalMaxima(sig2[vals[v_i]:vals[v_i + 1]]))\n",
        "        \n",
        "#         first_peaks1, first_peaks2  = closeby_pnts(pulse_peaks1[0], pulse_peaks2[0], 60)\n",
        "        \n",
        "        if len(pulse_peaks1)!=0 and len(pulse_peaks2)!=0:\n",
        "            valid_first_peaks1.append(pulse_peaks1[0] + vals[v_i])\n",
        "            valid_first_peaks2.append(pulse_peaks2[0] + vals[v_i])\n",
        "            \n",
        "    return np.array(valid_first_peaks1), np.array(valid_first_peaks2)\n",
        "\n",
        "\n",
        "def get_preprocessed_sig(sig, samp_freq, order, inpln_method):\n",
        "    \n",
        "    fs = samp_freq   \n",
        "\n",
        "    lowcutoff = 1    \n",
        "    highcutoff = 5  \n",
        "    order = 3       \n",
        "\n",
        "    notch_sig = adp_notch_filter(sig, fs)\n",
        "\n",
        "    bpass_sig = butter_bandpass_filter(notch_sig, lowcutoff, highcutoff, fs, order)\n",
        "    \n",
        "    peak_envpe, val_envpe, processed_sig = get_BW_removed_sig(bpass_sig, fs, inpln_method)\n",
        "    \n",
        "    preprocessed_sig = processed_sig / (peak_envpe - val_envpe)\n",
        "    \n",
        "    return peak_envpe, val_envpe, preprocessed_sig\n",
        "\n",
        "def drop_nan(x):\n",
        "    return x[~np.isnan(x)]\n",
        "\n",
        "# ******************************************  Store PTT values in excel ***************************************** #\n",
        "\n",
        "print(\"enter the file path\")\n",
        "user_fp = input()\n",
        "file_path = str(user_fp)\n",
        "\n",
        "# file_path = r\"E:\\Ramu_Pittala\\Multi Wavelength PTT Analysis\\Good Signals PTT Analysis\\48.2_110522_BP1_V_420_6_171.csv\"\n",
        "\n",
        "df=pd.read_csv(file_path,skiprows=36, header = None)\n",
        "\n",
        "# print(\"enter excel file key\")\n",
        "# excel_fp = input()\n",
        "# excel_file_key = excel_fp\n",
        "\n",
        "# wb = gc.open_by_key(excel_file_key).sheet1\n",
        "file_name = file_path.split(\"/\")[-1][:-4]\n",
        "sh = gc.create(file_name + \"_ptt_sheet\")\n",
        "wb = sh.sheet1\n",
        "# wb = sh.add_worksheet(title=\"ptt combination sheet\", rows=100, cols=20)\n",
        "\n",
        "fs = int(file_path.split(\"_\")[-3])\n",
        "num_measures = int(file_path.split(\"_\")[-2])\n",
        "\n",
        "if num_measures == 6:\n",
        "    column_ind = [1, 5, 9, 13, 17, 21]\n",
        "    mm_names = ['Meas1', 'Meas2', 'Meas3', 'Meas4', 'Meas5', 'Meas6']\n",
        "    \n",
        "elif num_measures == 4:\n",
        "    column_ind = [1, 5, 9, 13]\n",
        "    mm_names = ['Meas1', 'Meas2', 'Meas3', 'Meas4']\n",
        "    \n",
        "elif num_measures == 2:\n",
        "    column_ind = [1, 5]\n",
        "    mm_names = ['Meas1', 'Meas2']\n",
        "    \n",
        "ptt_names = ['PTT with First Peaks', 'PTT with Max Slopes', 'PTT with Valleys']\n",
        "    \n",
        "bw_length = len(column_ind) + 2\n",
        "\n",
        "row_ptt_mean_first_peaks = []\n",
        "row_ptt_mean_max_slopes = []\n",
        "row_ptt_mean_valleys = []\n",
        "\n",
        "for i in range(3):\n",
        "    row = 2\n",
        "    p = 2 + bw_length*i\n",
        "    q = p\n",
        "    wb.update_cell(row-1, p, ptt_names[i])\n",
        "    c = 0\n",
        "    d = row + 1\n",
        "    for j in range(p, p+num_measures):\n",
        "        wb.update_cell(row,j, mm_names[c])\n",
        "        wb.update_cell(d, q-1, mm_names[c])\n",
        "        c += 1\n",
        "        d += 1\n",
        "\n",
        "row = 3\n",
        "col = 3\n",
        "\n",
        "for r in range(len(column_ind)-1):\n",
        "    \n",
        "    col_ptt_mean_first_peaks = []\n",
        "    col_ptt_mean_max_slopes = []\n",
        "    col_ptt_mean_valleys = []\n",
        "    \n",
        "    wb.update_cell(row,col-1, 0)\n",
        "    wb.update_cell(row,col + bw_length-1 ,0)\n",
        "    wb.update_cell(row,col + (bw_length*2)-1, 0)\n",
        "    \n",
        "    for c in range(r+1, len(column_ind)):\n",
        "        \n",
        "        sig_1 = drop_nan(np.array(df.iloc[:, column_ind[r]])*(-1))\n",
        "        sig_2 = drop_nan(np.array(df.iloc[:, column_ind[c]])*(-1))\n",
        "\n",
        "        # ======================  Get different fiducial points ====================== ###\n",
        "\n",
        "        # :::::::::::::::::  Get pre-processed signals :::::::::::::::::::::::::::::: #\n",
        "        val_epe1, peak_epe1, proc_sig1 = get_preprocessed_sig(sig_1, fs, 3, \"quadratic\")\n",
        "        val_epe2, peak_epe2, proc_sig2 = get_preprocessed_sig(sig_2, fs, 3, \"quadratic\")\n",
        "\n",
        "        vals1, vals2 = get_valleys(proc_sig1, proc_sig2, fs)\n",
        "        sig_peaks1, sig_peaks2 = get_valid_first_peaks(vals1, proc_sig1, proc_sig2) # Here we need to provide good signal valleys as reference\n",
        "        sig_max_slopes1, sig_max_slopes2 =  get_valid_first_peaks(vals1, np.diff(proc_sig1), np.diff(proc_sig2))\n",
        "        \n",
        "        # =====================  PTT with different fiducial points ================= ###\n",
        "\n",
        "        ptt_first_peaks = np.mean(np.array(sig_peaks1) - np.array(sig_peaks2))\n",
        "\n",
        "        ptt_max_slopes = np.mean(np.array(sig_max_slopes1) - np.array(sig_max_slopes2))\n",
        "\n",
        "        ptt_valleys = np.mean(vals1 - vals2)\n",
        "        \n",
        "        col_ptt_mean_first_peaks.append(round(ptt_first_peaks,3))\n",
        "        wb.update_cell(row,col,round(ptt_first_peaks,3))\n",
        "        \n",
        "        col_ptt_mean_max_slopes.append(round(ptt_max_slopes,3))\n",
        "        wb.update_cell(row,col + bw_length ,round(ptt_max_slopes,3))\n",
        "        \n",
        "        col_ptt_mean_valleys.append(round(ptt_valleys,3))\n",
        "        wb.update_cell(row,col + bw_length*2,round(ptt_valleys,3))\n",
        "        \n",
        "        col += 1\n",
        "        \n",
        "    row += 1\n",
        "    col = row\n",
        "        \n",
        "    row_ptt_mean_first_peaks.append(col_ptt_mean_first_peaks)\n",
        "    row_ptt_mean_max_slopes.append(col_ptt_mean_max_slopes)\n",
        "    row_ptt_mean_valleys.append(col_ptt_mean_valleys)\n",
        "    \n",
        "wb.update_cell(row, col-1, 0);\n",
        "wb.update_cell(row,col + bw_length-1, 0);\n",
        "wb.update_cell(row,(col + bw_length*2)-1, 0);\n",
        "\n",
        "print(\"values are written :)\")"
      ],
      "metadata": {
        "id": "23yEV9LCauNq"
      },
      "id": "23yEV9LCauNq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sh.share('ramu@paretotree.com', perm_type='user', role='writer')\n",
        "fig, axs = plt.subplots(2, figsize = (10, 4))\n",
        "axs[0].plot(proc_sig1, label = \"Blue\")\n",
        "axs[0].plot(sig_peaks1, proc_sig1[sig_peaks1], 'x')\n",
        "axs[0].plot(vals1, proc_sig1[vals1], 'o')\n",
        "axs[0].plot(proc_sig2, label = \"Green\")\n",
        "axs[0].plot(sig_peaks2, proc_sig2[sig_peaks2], 'x')\n",
        "axs[0].plot(vals2, proc_sig2[vals2], 'o')\n",
        "axs[0].legend()\n",
        "\n",
        "# axs[1].plot((sig_b - min(sig_b))/(max(sig_b) - min(sig_b)) , label = \"Blue\")\n",
        "# axs[1].plot((sig_g - min(sig_g))/(max(sig_g) - min(sig_g)), label = \"Green\")\n",
        "axs[1].plot(proc_sig1, label = \"Signal 1\")\n",
        "axs[1].plot(proc_sig2, label = \"Signal 2\")\n",
        "axs[1].legend()\n",
        "# axs[1].plot(valleys_B, bpass_sigB[valleys_B], 'o')\n",
        "\n",
        "# plt.plot(valleys_G, bpass_sigG[valleys_G], 'o')\n",
        "# plt.plot(sig_peaksG, bpass_sigG[sig_peaksG], 'x')\n",
        "# plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GmKEJNJXbzs4"
      },
      "id": "GmKEJNJXbzs4",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-vFqsPeuk6mF"
      },
      "id": "-vFqsPeuk6mF",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Copy of Final_PD_code_for_good_signals.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}